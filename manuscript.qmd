---
title: "Global evidence suggests new directions for studying trust in science"
shorttitle: "Global evidence new directions"
author:
  - name: Jan Pfänder
    corresponding: false
    orcid: 0009-0009-4389-2807
    affiliations:
      - id: id1  # Added an explicit ID for referencing
        name: Swiss Federal Institute of Aquatic Science and Technology (Eawag)
        department: Department of Environmental Social Sciences
  - name: Viktoria Cologna
    corresponding: true
    orcid: 0000-0003-3706-8669
    email: viktoria.cologna@eawag.ch
    affiliations: 
      - ref: id1  # Now correctly references Jan Pfänder's affiliation
  - name: Niels Mede
    corresponding: false
    orcid: 0000-0001-5707-7568
    affiliations:
        name: Wageningen University & Research
abstract: "TBD"
keywords: [trust, science]
author-note:
  disclosures:
    conflict of interest: The authors have no conflict of interest to declare.
bibliography: references.bib    
format:
  apaquarto-docx: default
  apaquarto-html: default
  apaquarto-pdf: default
  apaquarto-typst: default
csl: elsevier-vancouver.csl
---

To recognize and effectively address some of the pressing challenges we face as humanity, we need to trust scientists: we cannot observe climate change or viruses ourselves, but have to rely on the analyses of scientists. People who trust scientists more are more likely to recognize human made climate change [@bogertEffectTrustScience2024] and to engage in pro-environmental behavior [@colognaRoleTrustClimate2020; @hornseyMetaanalysesDeterminantsOutcomes2016]. They are also more willing to get vaccinated [@sturgisTrustScienceSocial2021; for Covid-19 in particular, see @lindholtPublicAcceptanceCOVID192021].

Much research on trust in science has focused on the United States (US) and Europe. However, tackling climate change or pandemics require trust in science at a global scale. Although some global surveys exist [e.g., @wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020], many world regions remain much less systematically studied. "Manylabs" studies offer one promising approach to address this gap by pooling resources and expertise from research teams worldwide, allowing for more feasible and harmonized data collection. A recent manylabs study with over 71,000 participants from 68 countries illustrates the benefits of this approach by revealing important regional differences [@colognaTrustScientistsTheir2025].

Yet, simply broadening the geographical focus is not enough to advance our understanding of trust in science and its determinants. Global studies have produced partly inconsistent findings. For example, the @wellcomeglobalmonitorWellcomeGlobalMonitor2018 reported that greater income inequality was negatively associated with trust in scientists. By contrast, @colognaTrustScientistsTheir2025 found higher trust in countries with greater income inequality. Numerous differences in research design–from measuring trust to country selection–between the two studies make it difficult to compare the results and to draw conclusions.

As the field is growing and more knowledge is produced, it is important to consider how we study trust in science, so that evidence can be cumulative and robust. In this review, we summarize current approaches and highlight challenges on two levels: theoretical and methodological. We conclude with some normative considerations for future research.

# Theoretical level

When investigating public trust in science, researchers first need to ask: what exactly do we want to explain? Being as precise as possible about this question will improve comparability across studies and likely produce more robust results.

## Trust in what?

The term science can evoke different representations among different individuals or groups [@gauchatCulturalAuthorityScience2011]. Science may be understood as a body of literature, an institution, a method, individual scientists, disciplines, or even specific pieces of knowledge. These representations matter for trust. For example, in the US, people tend to trust scientific methods more than scientific institutions—particularly among less-educated segments of the population [@achterbergScienceConfidenceGap2017].

Trust in science is also domain-specific: people do not trust all of science equally. Domains may refer to groups of scientists. For instance, people around the world perceive climate scientists as less trustworthy than scientists in general [@ghasemiGapsPublicTrust2025]. Domains can also refer to scientific disciplines. In the US [@altenmullerExplainingPolarizedTrust2024a; @gligoricHowSocialEvaluations2024; @gauchatCulturalCognitiveMappingScientific2018], but also in France [@pfanderFrenchTrustMore2025], disciplines such as biology or physics enjoy considerably more trust than economics or sociology. Scientific knowledge is another example of domain-specific trust. In the US, essentially everyone--even a minority of people who say they do not trust science in general--tends to overwhelmingly trust basic scientific knowledge [e.g. electrons are smaller than atoms, @pfanderQuasiuniversalAcceptanceBasic2025]. Yet, on contentious topics, substantial minorities reject the scientific consensus [@pewresearchcenterMajorGapsPublic2015; on climate change, see, e.g., @stockemerUnderstandingClimateChange2024; on GMOs, see, e.g., @uscinskiHaveBeliefsConspiracy2022].

Despite this complexity, many large survey projects assess trust in science with very general questions. For example, the US General Social Survey has tracked trust in science for 50 years, but relies on a single item: “Would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in the scientific community?”[^1] Because people can hold different representations of science, and because trust is domain-specific, such general questions risk producing inconsistent findings across populations. It remains unclear what exactly these questions capture [@besleyReassessingVariablesUsed2021a; @besleyWhatAreYou2023].

[^1]: Similarly, the Pew research center relies on a single-item question, asking people about how much confidence they have that “scientists act in the best interests of the public” [@kennedyPublicTrustScientists2024].

## Trustees vs. trustors \[I don't know where this section is supposed to go, probably remove it\]

Trust relationships involve two sides: trustors, those who place trust, and trustees, those who are trusted [@mayerIntegrativeModelOrganizational1995a]. The side researchers focus on shapes their perspective on the roots of (dis)trust in science, and on possible interventions.

Work in science communication has typically emphasized the trustees—the scientists. Scholars have identified multiple dimensions of scientists’ trustworthiness, usually including an epistemological and an ethical dimension [@intemannScienceCommunicationPublic2023]. For example, @hendriksMeasuringLaypeoplesTrust2015 proposed three dimensions—expertise, integrity, and benevolence—while @besleyReassessingVariablesUsed2021a added openness as a fourth. Across these dimensions, competence is generally the dimension on which scientists are rated highest by the public [in the US, see @kennedyPublicTrustScientists2024; for global evidence, see @colognaTrustScientistsTheir2025].

By contrast, focusing on the trustors—the public—highlights the role of values, worldviews, and identities in shaping attitudes toward science [@hornseyAttitudeRootsJiu2017a]. Certain psychological traits, such as social dominance orientation [@hornseyWhyFactsAre2020] or a tendency toward conspiracy thinking [@rutjensConspiracyBeliefsScience2022], are linked to science rejection. Other research suggests that motivated reasoning–selecting and interpreting information to match one's existing beliefs or behaviors–could be a key driver of science rejection [@lewandowskyWorldviewmotivatedRejectionScience2021].

These two perspectives imply different roots of (dis)trust. A focus on trustees emphasizes factors endogenous to science—how scientists act or present themselves—suggesting that communication strategies may strengthen public trust [e.g., @besleyStrategicScienceCommunication2019]. A focus on trustors emphasizes factors exogenous to science—rooted in individuals’ psychology and social identities—where communication-based interventions may be less effective.

## Trustworthiness vs. trusting

Asking people broad questions about their trust in science not only mingles different representations of science; it also blurs the line between perceptions of trustworthiness and acts of trusting. Most research on public trust in science investigates the former—perceptions of scientists’ trustworthiness—while much less attention has been paid to the latter—behavioral trust [@besleyWhatAreYou2023].

Perceptions of trustworthiness are often measured by asking people to evaluate scientists’ character traits, such as competence, integrity, or benevolence [@hendriksMeasuringLaypeoplesTrust2015]. Behavioral trust, by contrast, concerns people’s willingness to accept vulnerability in relation to scientists. For instance, @besleyReassessingVariablesUsed2021a developed a scale to capture how much influence people think scientists should have over public policy and their personal life. This willingness to be vulnerable correlates with trust in scientists [@colognaTrustScientistsTheir2025].

Yet considerable conceptual work remains. Following @besleyWhatAreYou2023, wanting more political power for climate scientists would be an act of trusting. But scientists' role in society consists primarily in producing knowledge. Isn't then believing in human-made climate change already an act of trusting? If so, how can we distinguish between different forms of trusting behavior? Addressing these questions is essential for understanding how, and under what conditions, perceptions of trustworthiness translate into specific acts of trusting.

## Trust vs. distrust

Work on public trust in science typically seeks to explain why people do not trust science (enough). In the context of trust in political institutions, research has shown that trust and distrust are not necessarily symmetrical, and require different measurement approaches [@bertsouRethinkingPoliticalDistrust2019]. However, trust in science mostly relies on scales intended to capture trust. These scales make it hard to distinguish between a (passive) lack of trust and (active) distrust.

# Methodological level

On a methodological level, sample selection and measurement influence findings on trust in science.

## Sample selection

In the Global North, research on public perceptions of science is often institutionally anchored in public surveys, such as the National Science Foundation’s (NSF) “Science and Engineering Indicators” in the US, or the "Eurobarometer" in the EU. However, only studying these world regions risks biasing our theories about trust in science and its determinants, as we will illustrate with two examples: political orientation and religiosity.

In the US, there is a strong partisan divide regarding trust in science: democrats tend to express more trust in science, and republicans less [@krauseTrendsAmericansTrust2019a; @druckmanContinuityChangeTrust2024]. This suggests that political orientations are important determinant of trust. At a global scale, however, the picture is more nuanced. While in Europe and in the US more right (vs. left) leaning and more conservative (vs. liberal) individuals tend to trust scientists less, the opposite is true in several countries in Africa and Southeast Asia [@colognaTrustScientistsTheir2025]. Moreover, the role of political orientation can shift across domains of science: For instance, political conservatism is associated with greater climate change skepticism in North America and Europe, but not in other countries [@rutjensScienceSkepticism242022; @hornseyRelationshipsConspiratorialBeliefs2018].

A similar pattern emerges for religiosity. Studies have found that in the US [@azevedoIdeologicalBasisAntiscientific2021] or in the Netherlands [@rutjensSpiritualSkepticismHeterogeneous2020], more religious individuals express lower trust in scientists. By contrast, global evidence shows that religiosity is, on average, slightly positively associated with trust in scientists, driven mostly by a positive association in several Muslim countries [@colognaTrustScientistsTheir2025]. These results align with other global findings on religion and attitudes towards science more generally [@mcphetresReligiousAmericansHave2021].

These examples illustrate how alleged determinants of trust in science can be highly context-dependent. Relying primarily on Global North samples risks overgeneralization, whereas incorporating more diverse samples allows for more refined, and ultimately more accurate, explanations of (dis)trust in science.

## Measurement

Using different measures to assess the same theoretical construct can lead to inconsistent results. This concerns both trust in science and the covariates which researchers relate to trust.

Recent global studies on public perceptions of science have often relied on index measures of trust. For example, the Wellcome Global Monitor surveys use a five-item index to assess trust in scientists [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020][^2]. Index measures help address some concerns about general, single-item questions discussed above in the theoretical section. What they cannot address, however, is the issue of comparability: if studies use different trust indices and arrive at different conclusions, it is difficult to reconcile the discrepancies. As noted in the introduction, this problem is illustrated by conflicting findings on the relationship between trust in scientists and income inequality.

[^2]: These items are: "How much do you trust scientists in this country? A lot, some, not much or none at all?"; "In general, how much do you trust scientists to find out accurate information about the world? A lot, some, not much or none at all?"; "How much do you trust scientists working in colleges/universities in this country to do their work with the intention of benefiting the public? A lot, some, not much or none at all?"; "How much do you trust scientists working in colleges/universities in this country to be open and honest about who is paying for their work? A lot, some, not much or none at all?"; "How much do you trust scientists working for companies in this country to do their work with the intention of benefiting the public? A lot, some, not much or none at all?"

To increase comparability, researchers should be more specific about the construct they seek to capture and, ideally, converge on shared, validated scales. For example, @colognaTrustScientistsTheir2025 assessed the perceived trustworthiness of scientists with a multi-item index covering competence, benevolence, integrity, and openness. This more detailed scale allowed for fine-grained insights: While the public strongly endorsed scientists’ competence, perceptions of integrity and openness were notably weaker. These findings point to a key challenge for science communication—not only demonstrating expertise but also moral credibility and relatability.

Measurement divergence also applies to covariates commonly studied in relation to trust. Sometimes, new operationalizations can strengthen confidence in established findings. For instance, classic research in the field of public understanding of science found that the link between scientific knowledge and attitudes toward science is weak, and largely absent for issue-specific attitudes [@allumScienceKnowledgeAttitudes2008]. Using different measures, @colognaTrustScientistsTheir2025 confirmed this result: instead of assessing general attitudes toward and mesuring science knowledge with narrow factual quizzes, they focused on trust in scientists specifically, and used average national scores from the Program for International Student Assessment (PISA). They found no statistically significant association between countries' average trust in scientists their average PISA scores.

At the same time, measuring covariates differently can also introduce confusion. For example, many global studies have identified education—and particularly science education—as one of the strongest correlates of trust in science [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @noyScienceGoodEffects2019]. By contrast, @colognaTrustScientistsTheir2025 tested this relationship using only a coarse distinction between tertiary versus all other forms of education (including none), finding only a weak positive association. Had education been categorized in line with other studies, results might have been more comparable.

# Normative considerations

As research on trust in science increasingly relies on more culturally diverse samples, our understanding of regional patterns of trust and distrust has improved. This is valuable, but—as we have argued throughout this review—it is not sufficient on its own. For findings to be comparable across contexts and for theory to advance, we also need greater conceptual clarity and agreement on shared measures. Collaborative efforts, such as manylabs studies, provide promising pathways to achieve this.

Our approaches to studying trust in science are not neutral: they are guided, implicitly or explicitly, by normative assumptions about what trust in science should look like. Moving forward, we believe the field needs to reflect more explicitly on its normative foundations.

One important example is the long-standing influence of a deficit thinking. For decades, research on public perceptions of science has emphasized various public “deficits”—from a lack of knowledge to a lack of trust [@bauerWhatCanWe2007; @scheufeleThirtyYearsScience2022]. This deficit thinking, we believe, has two major consequences. First, it takes trust in science for granted, as a rational default. Theoretical approaches in psychology [@hornseyAttitudeRootsJiu2017a; @rutjensConspiracyBeliefsScience2022; @lewandowskyWorldviewmotivatedRejectionScience2021], communication [@medeSciencerelatedPopulismConceptualizing2020; @medeWhoSupportsSciencerelated2022], political science [@druckmanThreatsSciencePoliticization2022], and sociology [@gauchatCulturalAuthorityScience2011; @gauchatLegitimacyScience2023] have focused mainly on why certain groups deviate from that baseline. More research is needed on why people across the globe tend to trust science [@colognaTrustScientistsTheir2025], in spite of knowing little about it [@nationalacademiesofsciencesengineeringandmedicineScienceLiteracyConcepts2016]. Addressing this gap is essential if we want to understand how trust can be fostered and sustained [Potentially add preprint of theory paper with Hugo]. Second, deficit thinking suggests that the more trust in science the better. For many established topics of scientific consensus–e.g., the existence of human made climate change, or the safety of vaccines and GMOs–more trust in science can indeed be expected to better for society. But trust in science should not be blind, either: for disruptive knowledge and emerging technologies to be in the best public interest, the public needs to critically evaluated them [see e.g., @scheufeleWhatWeKnow2021]. A normative research agenda on trust should therefore not only ask how to increase trust, but also when healthy skepticism is warranted.

In sum, we believe that moving forward, in science research lies needs to integrate three levels of reflection: theoretical clarity about what trust is, methodological rigor to measure it consistently across contexts, and normative awareness of what levels and forms of trust are desirable. Elevated levels of public trust in science should not encourage complacency. In the US, too, most people trust science at least to some extent–yet, this year, the incumbent government has withdrawn from the World Health Organization [@thewhitehouseWithdrawingUnitedStates2025] as well as from the Paris Agreement on climate change [@thewhitehousePuttingAmericaFirst2025]; introduced massive budget cuts for scientific institutions [@bedekovicsMappingFederalFunding2025], arguing, ironically, that U.S. universities have “lost the public’s trust” [@mervisTrumpsScienceAdviser2025]; successfully sued universities [@drenonColumbiaUniversityPay2025]; frozen already allocated research grants [@mineoFreezingFundingHalts2025]; and interfered with the publishing of scientific research [@ajasaEPATellsScientists2025]. Advancing research on public trust in science is more important than ever.

# References
