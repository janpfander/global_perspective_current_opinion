---
title: "Global evidence suggests new directions for studying trust in scientists"
format: docx

abstract: |

  A recent global study has challenged several widely accepted findings in the literature on trust in science. We take this as an opportunity to highlight key theoretical, methodological, and normative issues that may contribute to inconsistent results in the field. To overcome these issues and advance research on trust in science, we suggest that scholars should: (i) sharpen theoretical precision by clearly specifying what aspect of trust they aim to explain; (ii) strengthen methodological rigor by drawing on more diverse samples and employing validated measurement scales; and (iii) broaden normative perspectives by moving beyond viewing the public as deficient and scientists as neutral actors.

bibliography: references.bib
---

\[change title to: "Global evidence suggests new directions for studying trust in SCIENCE"?\]

Survey research on public attitudes towards science dates back at least until the 1980 [@bauerWhatCanWe2007]. But some widely established findings regarding public trust in science have recently been challenged by a global study with over 71,000 participants from 68 countries [@colognaTrustScientistsTheir2025].

For example, the idea that science knowledge is the most important determinant of trust in science has long dominated the field [@allumScienceKnowledgeAttitudes2008][^1]. @colognaTrustScientistsTheir2025 tested the relationship between national science literacy scores, based on the Program for International Student Assessment (PISA), and national average trust in scientists for the 68 countries included in their study. They found no statistically significant association.

[^1]: Past research has already established knowledge of science is at best weakly correlated to attitudes towards science. However, this research has (i) looked at attitudes more generally, and not trust specifically, and (ii) has relied on a narrow set of science facts to measure science knowledge. Measuring trust in science and using PISA scores, @colognaTrustScientistsTheir2025 address these issues.

Similarly, education, and in particular science education, has been identified as one of the strongest correlates of trust in science in global samples [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @noyScienceGoodEffects2019]. Yet, @colognaTrustScientistsTheir2025 only find a weak positive relationship between tertiary education and trust in science.

Past studies which have focused on the Global North have found religiosity to be associated with lower trust in scientists [@azevedoIdeologicalBasisAntiscientific2021; @rutjensSpiritualSkepticismHeterogeneous2020; @mcphetresReligiousAmericansHave2021]. By contrast, @colognaTrustScientistsTheir2025 have found that, across the globe, religiosity is on average positively associated with trust in scientists.

Greater income inequality has been shown to be negatively associated with trust in science [@wellcomeglobalmonitorWellcomeGlobalMonitor2018]. By contrast, @colognaTrustScientistsTheir2025 found that trust in scientists is actually *higher* in countries with greater income inequality.

What to make of these inconsistent findings in the literature? One possibility is that trust in science–and its relationships with other variables–is highly volatile. More likely, however, is that the inconsistent findings reflect inconsistencies in research design. The aim of this article is to identify potential sources of these inconsistencies, highlight current shortcomings, and propose new research avenues on three levels: a theoretical, a methodological and a normative level.

# Theoretical level

When investigating public trust in science, researchers first need to ask the question: what exactly do we want to explain? Being as precise as possible about this question will improve comparability and likely generate more robust results.

## Trust in what?

The term science can evoke different representations among different individuals or groups [@gauchatCulturalAuthorityScience2011]. Science can be seen, for example, as a body of literature, an institution, a method, certain individual scientists, disciplines or specific pieces of knowledge. These representations of science matter for people's trust: For instance, it has been shown that people in the US tend to trust scientific methods more than scientific institutions—particularly among less-educated segments of the population [@achterbergScienceConfidenceGap2017].

In general, trust in science is not domain-general (people don't trust all of science equally), but domain specific. Domains can be for example different groups of scientists: people tend to trust some smore than others: Around the world, people perceive climate scientists as less trustworthy than scientists in general [@ghasemiGapsPublicTrust2025]. Domains can also be different scientific disciplines: in the US, people trust some disciplines, such as biology or physics, considerably more than others, such as economics or sociology [@altenmullerExplainingPolarizedTrust2024; @gligoricHowSocialEvaluations2024; @gauchatCulturalCognitiveMappingScientific2018]. Another example of domain-specific trust is scientific knowledge: people in the US–even those who say they do not trust science in general–tend to overwhelmingly trust basic scientific knowledge [e.g. electrons are smaller than atoms, @pfanderQuasiuniversalAcceptanceBasic2025], while on certain contentious topics, substantial minorities of the population reject the scientific consensus [@pewresearchcenterMajorGapsPublic2015; on climate change, see, e.g., @stockemerUnderstandingClimateChange2024; on GMOs, see, e.g., @uscinskiHaveBeliefsConspiracy2022].

To assess trust in science, many large scale survey ask general questions, such as "In general, would you say that you trust science a lot, some, not much, or not at all? \[1 = Not at all, 2 = Not much, 3 = Some, 4 =A lot\]" [@wellcomeglobalmonitorWellcomeGlobalMonitor2018][^2]. Because people can have different representations of science, and because trust in science is domain-specific, questions about science in general are prone to yield inconsistent findings–it is unclear, what exactly they measure [@besleyReassessingVariablesUsed2021a; @besleyWhatAreYou2023].

[^2]: Note that while the Wellcome Global Monitor includes this measure, its report focuses on an index measure that comprises several questions perceptions of scientists

## Trustees vs. trustors \[I don't know where this section is supposed to go, probably remove it\]

Trust relationships consist of two sides: trustors, those who place trust, and trustees, those who receive trust [@mayerIntegrativeModelOrganizational1995a]. Which of the two sides researchers focus on, result in different perspectives on the roots of (dis)trust in science, and what to do about it.

Work in science communication has focused on the trustees–the scientists. Researchers have proposed different dimensions of scientists' trustworthiness, the exact number of which can vary, but which typically cover an epistemological and an ethical dimension [@intemannScienceCommunicationPublic2023]. For example, @hendriksMeasuringLaypeoplesTrust2015 have argued for three dimensions–expertise, integrity, and benevolence–while @besleyReassessingVariablesUsed2021a has suggested openness as a fourth dimension. Across these dimensions, competence is typically the one on which scientists score highest in the perception of the public [in the US, see @kennedyPublicTrustScientists2024; for global evidence, see @colognaTrustScientistsTheir2025].

Focusing on the trustors, the public, work in psychology has shown that people's values, world views, and identities correlate with their attitudes towards science [@hornseyAttitudeRootsJiu2017]. Some research suggests that certain psychological traits, such as a social dominance orientation [@hornseyWhyFactsAre2020], or a tendency of engaging in conspiracy thinking [@rutjensConspiracyBeliefsScience2022], leads people to reject science. Other research suggests that motivated reasoning–selecting and interpreting information to match one's existing beliefs or behaviors–causes science rejection [@lewandowskyWorldviewmotivatedRejectionScience2021].

A focus on the trustees suggest that these roots are endogenous to science–how scientists act or present themselves influences public trust. Accordingly, research has proposed communication strategies for scientists to foster trust in science [e.g., @besleyStrategicScienceCommunication2019]. A focus on the trustors suggests that the roots are exogenous to science–they lie within people's psychology. From this perspectives, communication based interventions are less promising.

## Trustworthiness vs. trusting

Asking people general questions about their trust in science does not only mingle different representations of science; it also confuses perceptions of trustworthiness with acts of trusting. Research in public trust in science mostly investigates the former, perceptions of trustworthiness, but less the latter, behavioral trust [@besleyWhatAreYou2023]. Perceptions of trustworthiness can be measured, for example, by asking people to judge scientists' charter—e.g., their competence, integrity, or benevolence [@hendriksMeasuringLaypeoplesTrust2015]. To measure behavioral trust, @besleyReassessingVariablesUsed2021a developed a scale intended to capture people's "willingness to be vulnerable" to scientists, asking for example how much influence people believe scientists should have over public policy. This willingness to be vulnerable has been shown to correlate with trust in scientists [@colognaTrustScientistsTheir2025]. However, in our view, there is still a lot of conceptual work to be done regarding what exactly constitutes an act of trust in science: following @besleyWhatAreYou2023, wanting more political power for climate scientists is such an act. But is simply believing in human-made climate change already an act of trusting? And if yes, how do we differentiate between these forms of trusting? Answering this questions is necessary to understand better how and under which circumstances perceptions of trustworthiness translate into specific acts of trusting.

## Trust vs. distrust

Work on public trust in science typically seeks to explain why people do not trust science (enough). In the context of trust in political institutions, research has shown that trust and distrust are not necessarily symmetrical, and require different measurement approaches [@bertsouRethinkingPoliticalDistrust2019]. However, trust in science mostly relies on scales intended to capture trust. These scales make it hard to distinguish between a (passive) lack of trust and (active) distrust.

# Methodological level

Methodologically, biases in sample selection and inconsistencies in measurement are two key factors limiting the comparability of studies on trust in science.

## Sample selection

Much of the research on trust in science has focused on the United States (US) and Europe. In both regions, research on public perceptions of science is institutionally anchored in public surveys, such as the National Science Foundation’s (NSF) “Science and Engineering Indicators” in the US, or the "Eurobarometer" in the EU. Despite some large-scale global surveys [e.g., @wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020], other regions of the world remain much less systematically studied.

Which regions researchers choose to study trust in science is likely to impact their findings: For example, in the US, there is a significant partisan gap in trust in science: democrats tend to trust science more, and republicans tending to trust less [@krauseTrendsAmericansTrust2019a; @druckmanContinuityChangeTrust2024]. This observation, in isolation, might suggests that broader political orientations are important determinant of trust. At a global scale, however, the picture is more nuanced: While in Europe and in the US more right (vs. left) leaning and more conservative (vs. liberal) individuals tend to trust scientists less, the opposite is true in several countries in Africa and Southeast Asia [@colognaTrustScientistsTheir2025]. The relationship between political orientation and trust in science becomes even more nuanced when looking at specific areas of science: For instance, political conservatism is associated with greater climate change skepticism in North America and Europe, but not in other countries [@rutjensScienceSkepticism242022].

The example of political orientation illustrates how factors associated wit trust in science can be highly context-dependent. Relying on more diverse samples prevents from jumping to overgeneralizations and will ultimately lead to more refined explanations of (dis)trust in science.

## Measurement

Survey projects on public perceptions of science rely on very different measures of trust. For example, the US General Social Survey has measured trust in science for 50 years, but has used only one single item \["Would you say you have a great deal of confidence, only some confidence, or hardly any confidence at all in the scientific community?"\][^3]. As discussed before on the theoretical level, single-item, general measures of trust in science risk tapping into different representations of science among different people. This makes findings harder to compare. Other large-scale survey projects have addressed this issue by building index measures of trust: two Wellcome Global Monitors, for example, rely on an index measure of trust in scientists comprised of several items [@wellcomeglobalmonitorWellcomeGlobalMonitor2018; @wellcomeglobalmonitorWellcomeGlobalMonitor2020][^4]. These index measures, while accounting for different representations of science, differ between different survey projects, thus making it hard to compare the results. Researchers can overcome this by picking a specific theoretical construct they seek to measure, and agreeing on common, validated scales. For example, @colognaTrustScientistsTheir2025 measured trustworthiness of scientists with index of XX questions, covering dimensions of competence, benevolence, integrity, openness. Using this detailed scale allowed for fine-grained results. For example, while the public strongly believes in scientists’ competence, perceptions of integrity and openness are notably weaker. This highlights a key challenge for science communication—not just proving expertise, but also building moral credibility and relatability.

[^3]: Similarly, the PEW research center relies on a single-item question, asking people about how much confidence they have that “scientists act in the best interests of the public” [@kennedyPublicTrustScientists2024].

[^4]: These items are: "How much do you trust scientists in this country? A lot, some, not much or none at all?"; "In general, how much do you trust scientists to find out accurate information about the world? A lot, some, not much or none at all?"; "How much do you trust scientists working in colleges/universities in this country to do their work with the intention of benefiting the public? A lot, some, not much or none at all?"; "How much do you trust scientists working in colleges/universities in this country to be open and honest about who is paying for their work? A lot, some, not much or none at all?"; "How much do you trust scientists working for companies in this country to do their work with the intention of benefiting the public? A lot, some, not much or none at all?"

# Normative level

## How much trust in science is good?

Research on public perceptions of science has long operated in deficit paradigms, attesting the public various kinds of deficits–from a lack of scientific knowledge to a lack of trust in science [@scheufeleThirtyYearsScience2022]. Despite a recurrent rethoric of declining trust in science, cologna found that trust in science across the world, if not at ceiling, is moderately high. But how much trust in science is good?

For many established topics of scientific consensus–e.g., the existence of human made climate change, or the safety of vaccines and GMOs–more trust in science should result in better societal outcomes. But for disruptive knowledge and emerging technologies to be in the best public interest, the public also needs to critically evaluated them, rather than blindly trust the science [see e.g., @scheufeleWhatWeKnow2021].

Should people trust science blindly? That is unrealistic, because it depicts science as a univocal entity. That's the case for some things, and science is arguably very good at producing consensus. However, this has it's natural limits. Cutting edge science is controversial, with teams working against each other. Science could not replace democratic processes.

## Avoiding an over-focus (hyper-focus?) on trust

Trust in science helps in soliving societal issues from XX to XX. But trust is not always the main issue, and should not serve as a general diagnostic for all science related public issues. For example, communities of color in the United States and Covid-19 Vaccination – there are important historic issues that foster distrust. But maybe fixing structural issues (access to medication, poverty etc.) is in fact more important than thinking about communicative strategies of fixing trust.

## Beyond neutrality of scientists

Classic normative accounts of what makes science trustworthy have depicted the ideal scientists as neutral: an actor only committed to knowledge, impersonal (Merton), and making no value judgments (Weber). These ideals are often far from reality [see e.g., @colognaCommunicationValueJudgements2022], but they also do not necessarily correspond to what makes scientists trustworthy, in the eye of the public: Prior research shows that trust increases when people see scientists as sharing their values or social identity [@druckmanRepresentationScienceTrust2025]. The ideal of value-free scientists also suggests that scientists might risk their trustworthiness when publicly engaging in politics or advocacy. Contrasting this view, @colognaTrustScientistsTheir2025 find that most people do in fact want scientists to actively advocate for specific policies and be more involved in policymaking processes. Support is especially strong among younger, urban, and more educated respondents, though conservatives tend to be less favorable. Even if political engagement is not always viewed positively, it is important for guaranteeing transparency, as personal values bias researchers' policy recommendations [@pielkejrHonestBrokerMaking2007].

# Conclusion

In a recent meta-analysis, @cologna find that trust in science is moderatly high. However, that trust should not be taken for granted. Even distrusting of minorities, especially when in positions of power, can have devastating consequences for the relationship between science and society. In the US, too, most people trust science, at least to some extent. Yet, the incumbent government has this year denounced plans to tackle climate change, cancel vaccination programs, and introduce massive budget cuts for science.

Trust in science matters, but research comes to inconsistent results. We have argued that these inconsistencies are likely to be rooted in choices of research design on different levels (theoretical, methodological, normative). By showcasing some issues and shortcomings, we deem important, we hope to raise awareness among the research community for these inconsistencies. Hopefully, we could also lay out some paths to move forward.

## Some notes

-   @achterbergScienceConfidenceGap2017: They measured trust in scientific institutions as an index of reported (i) confidence in science, (ii) confidence in scientists, (iii) (dis)agreemtn with the statement that "scientific knowledge is nothing but an opinion". They measured trust in sicentific method with (dis)agreement to the questions ("Knowledge can only be obtained through unbiased systematic research")

-   In 2023, a survey in eight different countries found that up to 24% of respondents agreed that “climate change is a hoax and scientists touting its existence are lying” [@stockemerUnderstandingClimateChange2024]

-   In 2021, 40% of Americans believed that “the dangers of genetically-modified foods are being hidden from the public” [down from 45% in 2020, @uscinskiHaveBeliefsConspiracy2022]

-   A recent Pew survey found that 89% of Americans viewed research scientists as intelligent, but only 65% viewed them as honest [@kennedyPublicTrustScientists2024].

-   Scheufele: "Following this trust deficit argument, many in the scientific community have blamed COVID19 vaccine hesitancy and other disconnects between science and communities of color in the United States on low levels of trust, driven by historical atrocities, including not treating sick patients during the Tuskegee Syphilis Study or profiting, without consent, from cell lines obtained from African American patients. And while there is some truth in this diagnosis, we also know that expectant African American mothers in the United States today have a three times higher chance of dying during childbirth than White women (Krause et al., 2021), with similar discrepancies in health outcomes for women of color also emerging in countries like the United Kingdom (Knight et al., 2021). Addressing this discrepancy is not primarily a challenge of dealing with symptoms, that is, of rebuilding trust in the medical community. It is a challenge of eliminating the underlying cause, that is, addressing persistent health disparities based on class, race, and socioeconomic status."
